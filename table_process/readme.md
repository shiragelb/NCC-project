# Hebrew Statistical Table Processing Pipeline with BigQuery Integration
A comprehensive system for extracting, tracking, merging, and storing Hebrew statistical tables from multi-year government reports (2001-2024) in BigQuery for scalable analysis.

## üéØ Overview
This pipeline processes Hebrew statistical documents organized by year and chapter, extracting tables from Word documents, creating temporal chains that track how tables evolve across years, merging related datasets to produce complete time series data, and storing everything in BigQuery for efficient querying and analysis.

## üìä Pipeline Architecture
The system consists of six main stages:
```
Stage 1: EXTRACT TABLES (extract_tables/)
  ‚Üì Extracts tables from Word docs ‚Üí CSV files + metadata

Stage 2A: CREATE CHAINS (chain/table-chain-matching/)
  ‚Üì Matches tables across years ‚Üí temporal chains

Stage 2B: MERGE CHAINS (chain/chain-api-expantion/)
  ‚Üì Combines complementary chains ‚Üí consolidated chains

Stage 2C: MASK GENERATION (mask/)
  ‚Üì Classifies cells as features/data-points using rules + LLM

Stage 2D: BIGQUERY MIGRATION (data_structure/)
  ‚Üì Loads all data to BigQuery ‚Üí queryable database

Stage 3: FINAL PROCESSING (merge_chains/)
  ‚Üì Reads from BigQuery, aligns columns ‚Üí final datasets in BigQuery
```

## üóÑÔ∏è BigQuery Data Structure
The pipeline creates four main tables in BigQuery:

### chains_metadata
```sql
- chapter_id: INTEGER (1-15)
- chapter_name: STRING (Hebrew chapter title)  
- chain_id: STRING (e.g., "chain_1_01_2001")
- chain_name: STRING (Hebrew chain description)
- table_count: INTEGER
- years: ARRAY<INTEGER>
- gaps: ARRAY<INTEGER> (missing years)
```

### tables_data  
```sql
- chapter_id: INTEGER
- chain_id: STRING
- table_id: STRING (e.g., "1_01_2001")
- table_name: STRING (Hebrew table title)
- year: INTEGER
- row_index: INTEGER (row position in table)
- col_index: INTEGER (column position in table)
- cell_value: STRING (actual data value)
```
Note: Stored in "long format" - one row per cell

### masks_data
```sql
- chapter_id: INTEGER
- chain_id: STRING
- table_id: STRING
- row_index: INTEGER
- col_index: INTEGER
- is_feature: BOOLEAN (true = header/label, false = data value)
```
Note: Generated by mask pipeline using hard rules + LLM classification

### merged_chains (Output)
```sql
- chain_id: STRING
- chapter_id: INTEGER
- meta_year: INTEGER (year of the data)
- row_index: INTEGER
- column_name: STRING (merged column header)
- cell_value: STRING
- merge_status: STRING (pending/completed/failed)
- merge_timestamp: TIMESTAMP
```
Note: Automatically created and populated by merge_chains stage

## üìÅ Complete Data Flow

### üîµ Stage 1: Table Extraction (`extract_tables/`)
Downloads Word documents from Google Drive and extracts all tables with their Hebrew headers.
- **Output:** CSV files and `tables_summary.json`

### üîµ Stage 2A: Chain Creation (`chain/table-chain-matching/`)
Analyzes tables across all years to create temporal chains.
- **Output:** Chain definition files `chains_chapter_*.json`

### üîµ Stage 2B: Chain Merging (`chain/chain-api-expantion/`)
Merges complementary chains using Claude API for validation.
- **Output:** Consolidated chains with merged IDs

### üîµ Stage 2C: BigQuery Migration (`data_structure/`)
Loads all chains, tables, and masks to BigQuery database.
- **Creates:** All BigQuery tables with indexed data

### üîµ Stage 3: Final Processing (`merge_chains/`)
Reads data directly from BigQuery and creates final aligned datasets.
- **Input:** Data from BigQuery tables
- **Output:** Merged datasets in both CSV and BigQuery formats

## üöÄ Quick Start Guide

### Prerequisites
1. **Install Python 3.8+**
2. **Set up Google Cloud credentials:**
```bash
gcloud auth application-default login
export GCP_PROJECT_ID="ncc-data-bigquery"
```
3. **Install all dependencies:**
```bash
cd table_process
# Install base requirements
pip install -r requirements.txt
pip install google-cloud-bigquery db-dtypes python-Levenshtein
pip install anthropic  # For chain merging
```

### Option A: Full Pipeline (Extract ‚Üí Chain ‚Üí Migrate ‚Üí Merge)
```bash
# Step 1: Extract tables from documents
python extract_tables/main.py --drive-folder-id YOUR_ID --years 2021 2022 2023

# Step 2A: Create chains
cd chain/table-chain-matching && python main.py && cd ../..

# Step 2B: Merge chains  
cd chain/chain-api-expantion && python merge_chains_iterative.py --chapters 1 2 3 && cd ../..

# Step 2C: Migrate to BigQuery
python data_structure/final_migrate.py

# Step 3: Process from BigQuery
cd merge_chains && python run_pipeline.py --chapters 1 2 3
```

### Option B: Process Directly from BigQuery (if already migrated)
```bash
cd merge_chains
# Process specific chain (supports partial matching for merged chains)
python run_pipeline.py --chapters 1 --chains chain_1_01_2001

# Process all chains in chapters
python run_pipeline.py --chapters 1 2 3
```

## üîç Querying BigQuery

Once data is in BigQuery, you can run powerful queries:

```sql
-- View merged results for a specific chain
SELECT meta_year, column_name, cell_value 
FROM `ncc-data-bigquery.chains_dataset.merged_chains`
WHERE chain_id = 'merged_chain_1_01_2001_chain_1_01_2005'
AND merge_status = 'completed'
ORDER BY meta_year, row_index, column_name

-- Check merge status for all chains
SELECT chapter_id, COUNT(DISTINCT chain_id) as total_chains,
       SUM(CASE WHEN merge_status = 'completed' THEN 1 ELSE 0 END) as completed,
       SUM(CASE WHEN merge_status = 'pending' THEN 1 ELSE 0 END) as pending
FROM `ncc-data-bigquery.chains_dataset.merged_chains`
GROUP BY chapter_id

-- Get all data for a specific year across all tables
SELECT DISTINCT table_id, table_name 
FROM `ncc-data-bigquery.chains_dataset.tables_data`
WHERE year = 2021
```

## üìà Processing Statistics
- **Extract:** ~30 seconds per document
- **Chain Creation:** ~2-5 minutes per chapter
- **Chain Merging:** ~1-3 minutes per chapter  
- **BigQuery Migration:** ~2-3 hours for full dataset
- **Final Processing:** ~10-30 seconds per chain (from BigQuery)

## üéõÔ∏è Configuration

### BigQuery Dataset
- **Project:** `ncc-data-bigquery`
- **Dataset:** `chains_dataset`
- **Tables:** `chains_metadata`, `tables_data`, `masks_data`, `merged_chains`

### Environment Variables
```bash
export GCP_PROJECT_ID="ncc-data-bigquery"
export GOOGLE_CLOUD_PROJECT="ncc-data-bigquery"  # Alternative
export ANTHROPIC_API_KEY="sk-ant-api-..."  # For chain merging
export DRIVE_FOLDER_ID="your-drive-folder"  # For extraction/migration
```

### Key File Locations
- **Chain configs:** `merge_chains/chain_configs/chains_chapter_*.json`
- **Merged outputs:** `merge_chains/output/merged_*.csv`
- **Logs:** `merge_chains/logs/pipeline_*.log`

## üêõ Troubleshooting

### BigQuery Connection Issues
```bash
# Verify authentication
gcloud auth application-default print-access-token

# Set project explicitly
export GOOGLE_CLOUD_PROJECT=ncc-data-bigquery

# Test BigQuery access
bq ls -p ncc-data-bigquery
```

### Missing Dependencies
```bash
# Install all required packages
pip install pandas numpy google-cloud-bigquery db-dtypes     python-Levenshtein scikit-learn google-auth     google-auth-oauthlib google-auth-httplib2     google-api-python-client python-dotenv tqdm
```

### Chain Not Found
The pipeline now supports partial matching for merged chains. When specifying `--chains chain_1_01_2001`, it will match:
- Exact: `chain_1_01_2001`
- Merged: `merged_chain_1_01_2001_chain_1_01_2005`
- Any chain containing that ID

### Memory Issues with Large Queries
Process fewer chapters at once or specific chains:
```bash
python run_pipeline.py --chapters 1 --chains chain_1_01_2001
```

## üìù File Naming Conventions
- **Table IDs:** `{serial}_{chapter}_{year}` (e.g., `1_01_2021`)
- **Chain IDs:** `chain_{serial}_{chapter}_{year}`
- **Merged chain IDs:** `merged_chain_{id1}_chain_{id2}`
- **Final outputs:** `merged_{chain_id}.csv`

## üîÑ Data Updates
To update with new years of data:
1. Extract new tables (Stage 1)
2. Re-run chain creation (Stage 2A)  
3. Merge updated chains (Stage 2B)
4. Re-migrate to BigQuery (Stage 2C)
5. Process final outputs (Stage 3)

## ‚úÖ Verifying Pipeline Success
```bash
# Check extraction
ls chain/table-chain-matching/tables/

# Check chains created
ls chain/chain-api-expantion/chains_chapter_*.json

# Check BigQuery tables
bq query --use_legacy_sql=false "SELECT COUNT(*) FROM ncc-data-bigquery.chains_dataset.tables_data"

# Check merged outputs
ls merge_chains/output/merged_*.csv

# Check merge status in BigQuery
bq query --use_legacy_sql=false "
SELECT merge_status, COUNT(*) as count 
FROM ncc-data-bigquery.chains_dataset.merged_chains 
GROUP BY merge_status"
```

## üìö Additional Resources
- Each subdirectory contains its own detailed README
- Processing logs saved with timestamps in logs/ directories
- API usage tracked in merge reports

## ü§ù Contributing
When adding new features:
1. Update the appropriate stage's README
2. Ensure BigQuery compatibility
3. Test with a single chain first
4. Document any schema changes
5. Update this main README
