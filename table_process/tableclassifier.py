# -*- coding: utf-8 -*-
"""TableClassifier.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UhkTmVurw_BJeskuN4xTVQKuv1HsN1GN
"""

class TableClassifier:
    """Orchestrates the entire table classification process."""

    def __init__(self, api_key=None):
        """
        Initialize the complete classification system.

        Args:
            api_key: Anthropic API key (uses env var if None)
        """
        import pandas as pd
        import json
        import os
        import logging
        import glob
        from pathlib import Path
        from concurrent.futures import ThreadPoolExecutor, as_completed

        self.pd = pd
        self.json = json
        self.os = os
        self.logging = logging
        self.glob = glob
        self.Path = Path
        self.ThreadPoolExecutor = ThreadPoolExecutor
        self.as_completed = as_completed

        self.loader = TableLoader()
        self.hard_classifier = HardRuleClassifier()
        self.llm_classifier = LLMClassifier(api_key)

    def classify_table(self, csv_path: str, table_name: str):
        """
        Complete classification pipeline for a table.

        Args:
            csv_path: Path to the CSV file
            table_name: Name/title of the table for context

        Returns:
            Final mask DataFrame with classifications
        """
        # Step 1: Load and clean the table
        table_df = self.loader.load_and_clean(csv_path)

        # Step 2: Apply hard-coded rules
        partial_mask = self.hard_classifier.classify(table_df)
        # print(f"Partial mask shape: {partial_mask.shape}")

        # Step 3: Apply LLM classification for undecided cells only if cells are left "undecided"
        if "undecided" in partial_mask.values:
            final_mask = self.llm_classifier.classify_undecided(table_name, table_df, partial_mask)
        else:
            final_mask = partial_mask

        return final_mask

    def load_table_names(self, summary_path: str = "/content/tables_summary.json") -> dict:
        """
        Load table name mappings from the summary JSON file.

        Args:
            summary_path: Path to the tables_summary.json file

        Returns:
            Dictionary mapping CSV identifiers to table names
        """
        try:
            with open(summary_path, 'r', encoding='utf-8') as f:
                return self.json.load(f)
        except FileNotFoundError:
            print(f"Warning: {summary_path} not found. Using filenames as table names.")
            return {}
        except self.json.JSONDecodeError as e:
            print(f"Error parsing {summary_path}: {e}. Using filenames as table names.")
            return {}

    def process_single_table(self, input_path: str, output_path: str, table_name: str) -> bool:
        """
        Process a single table and save its classification mask.

        Args:
            input_path: Path to the input CSV file
            output_path: Path where the mask CSV should be saved
            table_name: Human-readable name of the table for LLM context

        Returns:
            True if processing succeeded, False otherwise
        """
        try:
            # print(f"Processing {os.path.splitext(os.path.basename(input_path))[0]}")
            # Process the table using existing classification pipeline
            mask_df = self.classify_table(input_path, table_name)

            # Save the mask to CSV
            mask_df.to_csv(output_path, index=False)

            return True

        except Exception as e:
            print(f"Failed to process {self.os.path.basename(input_path)}: {e}")
            return False

    def process_all_tables(self,
                          input_dir: str = "/content/drive/MyDrive/DSSG/tables/",
                          output_dir: str = "/content/mask",
                          summary_path: str = "/content/drive/MyDrive/DSSG/tables/tables_summary.json",
                          years=None,
                          chapters=None,
                          parallel: bool = False,
                          skip_existing: bool = True,
                          max_workers: int = 4) -> None:
        """
        Process all CSV files in the hierarchical directory structure and save masks to output directory.

        Args:
            input_dir: Base directory containing input CSV files organized by year/chapter
            output_dir: Base directory where mask CSV files will be saved
            summary_path: Path to the tables_summary.json file
            years: Iterable of years to process (list, range, numpy array, etc.) (default: 2001-2024)
            chapters: Iterable of chapters to process (list, range, numpy array, etc.) (default: 1-15)
            parallel: Whether to use parallel processing (default: False)
            skip_existing: Whether to skip already processed files (default: True)
            max_workers: Maximum number of parallel workers if parallel=True (default: 4)
        """
        # Set defaults if not provided
        if years is None:
            years = range(2001, 2025)
        if chapters is None:
            chapters = range(1, 16)  # Chapters 1-15

        # Load table name mappings
        table_names = self.load_table_names(summary_path)

        # Collect all files to process (for parallel processing or progress tracking)
        files_to_process = []

        # Iterate through year/chapter combinations

        for chapter in chapters:
            for year in years:
                chapter_year = f"{year}/{chapter:02d}"
                input_path = self.Path(input_dir) / chapter_year

                # Check if folder exists
                if not input_path.exists():
                    self.logging.warning(f"Folder does not exist: {input_path}")
                    continue

                # Get all CSV files in this folder
                csv_files = list(input_path.glob("*.csv"))

                if not csv_files:
                    self.logging.warning(f"No CSV files found in {input_path}")
                    continue

                # Process each CSV file
                for csv_file in csv_files:
                    # Get filename without extension for identifier lookup
                    identifier = csv_file.stem

                    # Get table name from mapping or use filename
                    table_name = table_names.get(identifier, identifier)

                    # Construct output path with same filename
                    output_path = self.Path(output_dir) / chapter_year / csv_file.name

                    # Check if we should skip this file
                    output_drive_path = self.Path("/content/drive/MyDrive/DSSG/mask/mask") / chapter_year / csv_file.name
                    if skip_existing and (output_path.exists() or output_drive_path.exists()):
                        self.logging.info(f"Skipping existing file: {identifier}")
                        continue

                    if parallel:
                        # Collect for parallel processing
                        files_to_process.append((str(csv_file), str(output_path), table_name))
                    else:
                        # Process immediately (sequential)
                        print(f"Processing: {identifier}")

                        # Ensure the output directory exists
                        self.ensure_output_directory(str(output_path.parent))

                        try:
                            # Process the table
                            self.process_single_table(str(csv_file), str(output_path), table_name)
                        except Exception as e:
                            self.logging.error(f"Error processing {identifier}: {str(e)}")
                            continue

        # If parallel processing is enabled, process all collected files
        if parallel:
            self.process_tables_parallel(files_to_process, max_workers)

    def process_tables_parallel(self, files_to_process: list, max_workers: int = 4) -> None:
        """
        Process tables in parallel using concurrent.futures.

        Args:
            files_to_process: List of tuples (input_path, output_path, table_name)
            max_workers: Maximum number of parallel workers
        """

        if not files_to_process:
            print("No files to process")
            return

        print(f"Processing {len(files_to_process)} files in parallel with {max_workers} workers")

        def process_file(args):
            input_path, output_path, table_name = args
            identifier = self.Path(input_path).stem

            try:
                # Ensure the output directory exists
                self.ensure_output_directory(str(self.Path(output_path).parent))

                # Process the table
                self.process_single_table(input_path, output_path, table_name)
                return f"Success: {identifier}"
            except Exception as e:
                return f"Error processing {identifier}: {str(e)}"

        with self.ThreadPoolExecutor(max_workers=max_workers) as executor:
            # Submit all tasks
            futures = {executor.submit(process_file, args): args for args in files_to_process}

            # Process completed tasks
            for future in self.as_completed(futures):
                result = future.result()
                if result.startswith("Error"):
                    self.logging.error(result)
                else:
                    print(result)

    def ensure_output_directory(self, output_dir: str) -> None:
        """
        Ensure the output directory exists, create it if necessary.

        Args:
            output_dir: Path to the output directory
        """
        self.Path(output_dir).mkdir(parents=True, exist_ok=True)

    def usage_summary(self):
        return self.llm_classifier.get_usage_summary()